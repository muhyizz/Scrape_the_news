{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56cdabf",
   "metadata": {},
   "source": [
    "# <center> Google News Web Scrapper With Sentiment Analysis </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ceeb1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "- The purpose of this project is to show my capability of developing a full stack data project \n",
    "</div>\n",
    "<div style=\"text-align: justify\"> \n",
    "- The project utilizes python [Web Scrapper, Data Cleaning, Sentiment Analysis and Connection Point to SQL Database],   Microsft SQL Server Management Studio [Data Storage], PowerBi [Visualization] and general problem solving and troubleshooting skill.\n",
    "</div>\n",
    "\n",
    "- The project is divided into multiple parts which are:\n",
    "1. Data Scrapping [Python - Selenium & Beautiful Soup]\n",
    "2. Data Cleaning and formating [Python - Pandas]\n",
    "3. Sentiment Analysis [Python - TextBlob]\n",
    "4. Data Storage [MSSMS]\n",
    "5. Data Visualization [PowerBi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59443695",
   "metadata": {},
   "source": [
    "### <center> Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6c4a061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries to be use during data scrapping.\n",
    "#Selenium is used to access google and send the subject matter that is studied.\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options #Future Improvement - headless option\n",
    "\n",
    "#Beautiful soup is used to scrape the exact information needed from the Google News Page\n",
    "#The reason why Beautiful soup is used instead of utilizing selenium as it has the same capability was down to ease of use\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Time is used to create a delay between page as additional buffer to selenium WebDriverWait to ensure all information is \n",
    "#accessible by beautiful soup.\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e04420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subject of analysis\n",
    "id = 'Malaysia' # Can be adjusted to be user input instead of predefined variable, currently optimized for country search\n",
    "\n",
    "#Initialising the web driver and navigating to the google\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "url = 'https://www.google.com/'\n",
    "driver.get(url)\n",
    "\n",
    "#Locating the search bar, inputting the subject of analysis, and navigating to the news page\n",
    "search_bar_ID = 'APjFqb'\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, search_bar_ID))).send_keys(id)\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, search_bar_ID))).send_keys(Keys.RETURN)\n",
    "time.sleep(5)\n",
    "WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'O3S9Rb'))).click() #Navigating to news page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad18601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page = 1 # The current page of news\n",
    "num_pages = 10  # Specify the number of news page to iterate \n",
    "\n",
    "# Identifying variable to store the scrapped information\n",
    "\n",
    "news_subject_clean = []\n",
    "news_outlet_clean = []\n",
    "time_passed_clean = []\n",
    "\n",
    "# Iterating through the pages using selenium and extracting the information using beautiful soup\n",
    "\n",
    "while current_page <= num_pages:\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find all elements with a specific class\n",
    "    \n",
    "    news_subject = soup.find_all(class_=\"n0jPhd ynAwRc MBeuO nDgy9d\")\n",
    "    time_passed = soup.find_all(class_ = \"OSrXXb rbYSKb LfVVr\")\n",
    "    news_outlet = soup.find_all(class_ = \"MgUUmf NUnG9d\")\n",
    "\n",
    "    # Due to the google news layout page which has predictive search subcategory of the subject\n",
    "    # we are required to remove it from our data scrapped\n",
    "    \n",
    "    sections = soup.find_all('div', jsname='K9a4Re')\n",
    "    for section in sections:\n",
    "        excluded_elements = section.find_all('div', class_='MgUUmf NUnG9d')\n",
    "        news_outlet = [element for element in news_outlet if element not in excluded_elements]\n",
    "        \n",
    "        \n",
    "    #Storing the data into the pre intialized variables\n",
    "    \n",
    "    for i, element in enumerate (news_subject):\n",
    "        news_subject_clean.append(element.get_text())\n",
    "\n",
    "    for i, element in enumerate (news_outlet):\n",
    "        news_outlet_clean.append(element.get_text())\n",
    "\n",
    "    for i, element in enumerate (time_passed):\n",
    "        time_passed_clean.append(element.get_text())\n",
    "        \n",
    "    #Navigating the next page button\n",
    "    next_button = driver.find_element(By.ID, \"pnnext\")\n",
    "    next_button.click()\n",
    "\n",
    "    #Iterating the pages\n",
    "    current_page += 1\n",
    "    time.sleep(2)\n",
    "\n",
    "driver.quit() #Closes the browser and driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b2df2",
   "metadata": {},
   "source": [
    "### <center> Data Cleaning and formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a6b1b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pandas to format the information into a table for further processes\n",
    "import pandas as pd\n",
    "\n",
    "#Importing date and time to include the date the information was extracted\n",
    "from datetime import date\n",
    "\n",
    "#pd.set_option('display.max_colwidth', None) - to remove character limit when displaying the data frame\n",
    "\n",
    "#Merging all the information into a data frame\n",
    "df = pd.DataFrame({'time_passed': time_passed_clean,'news_outlet': news_outlet_clean, 'news_subject': news_subject_clean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c0b7e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing html pragraph identifier\n",
    "\n",
    "df['news_subject'] = df['news_subject'].replace(['\\n'], '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f5138c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the text ago from time passed\n",
    "\n",
    "df['time_passed'] = df['time_passed'].replace(['ago'], '', regex=True)\n",
    "\n",
    "#This was done for future improvement of looping the content of the table and replacing with an integer value in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ac52ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timestamp of when the information is extracted\n",
    "\n",
    "df['date'] = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0c454d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearranging the columns for ease of processing\n",
    "\n",
    "df2 = df[['date','time_passed','news_outlet','news_subject']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac0b88",
   "metadata": {},
   "source": [
    "### <center> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b1e1ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing textblob for its sentiment analysis capabilities\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Other option include HuggingFace, Vader and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "69d8e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialising a variable to store the sentiment analysis\n",
    "polarity = []\n",
    "\n",
    "#Looping through the data frame on the 3rd column (news_subject) and appending the outcome into the variable polarity\n",
    "\n",
    "for i in range (len(df2)):\n",
    "    polar = ((TextBlob(df2.iloc[i,3])).sentiment.polarity)\n",
    "    if polar < 0:\n",
    "        polarity.append('Negative')\n",
    "    elif polar == 0:\n",
    "        polarity.append('Neutral')\n",
    "    else:\n",
    "        polarity.append('Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bfb0fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the polarity variable array into a data frame \n",
    "\n",
    "df_polar = pd.DataFrame({'Sentiment':polarity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fd904d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinating the target data frame (df2) and the sentiment polarity dataframe\n",
    "\n",
    "df_final = pd.concat([df2,df_polar], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3f108",
   "metadata": {},
   "source": [
    "### <center> Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "444f71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pyodbc as a connecting point to MSSMS\n",
    "import pyodbc\n",
    "\n",
    "#Other option include SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "728a66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the event that the table dosent exist in the server this query is defined to create the table\n",
    "create_table_query = \"\"\"\n",
    "--Create Table if it does not exist\n",
    "\n",
    "IF Object_ID('{subject}') IS NULL\n",
    "\n",
    "CREATE TABLE [pyodbc].[dbo].[{subject}]\n",
    "(\n",
    "[date] DATE,\n",
    "[time_passed] VARCHAR(50),\n",
    "[news_outlet] VARCHAR(50),\n",
    "[news_subject] VARCHAR(250),\n",
    "[sentiment] VARCHAR (30),\n",
    ")\n",
    "\n",
    "\"\"\".format(subject=id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4751b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This query is to inser the data into the table\n",
    "sql_insert = \"\"\"\n",
    "INSERT INTO [pyodbc].[dbo].[{subject}](\n",
    "    [date],\n",
    "    [time_passed],\n",
    "    [news_outlet],\n",
    "    [news_subject],\n",
    "    [sentiment]\n",
    ")\n",
    "VALUES\n",
    "(\n",
    "    ?,?,?,?,?\n",
    ")\"\"\".format(subject=id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fa0b2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the component of the connection string\n",
    "\n",
    "DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "SERVER_NAME = 'DESKTOP-50TGEJ8\\SQLEXPRESS'\n",
    "DATABASE_NAME = 'pyodbc'\n",
    "\n",
    "#This is based on the user server information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9edc0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the connection string\n",
    "\n",
    "CONNECTION_STRING = \"\"\"\n",
    "Driver={driver};\n",
    "Server={server};\n",
    "Database={database};\n",
    "Trusted_Connection=yes;\n",
    "\"\"\".format(\n",
    "    driver=DRIVER,\n",
    "    server=SERVER_NAME,\n",
    "    database = DATABASE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "619e31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the database\n",
    "\n",
    "connection_object = pyodbc.connect(CONNECTION_STRING)\n",
    "\n",
    "#Create a cursor object (pyodbc specific function)\n",
    "\n",
    "cursor_object = connection_object.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e2b8e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "cursor_object.execute(create_table_query)\n",
    "\n",
    "#Commit the table created\n",
    "cursor_object.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fcba1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataframe into a record set (dictionary) to process for SQL insert\n",
    "df_records = df_final.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e9b27d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting Value into table\n",
    "cursor_object.executemany(sql_insert,df_records)\n",
    "\n",
    "#Commit the data into the table\n",
    "cursor_object.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f00e85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End the connection to the server\n",
    "cursor_object.close()\n",
    "connection_object.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
